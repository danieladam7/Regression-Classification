# -*- coding: utf-8 -*-
"""Regression Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10csyIyMweJwkegWElUzutMvqfHks43cl

#**ML problem - producing most accurate regressor on abstract data**

**Pre experiment phase:**
- 1) Preliminary analysis
  - 1.1) print data
  - 1.2) Visualization
  - 1.3) Indicating presence of NaN values in dataset
  - 1.4) Range of (continous) target values
  - 1.5) Number of samples and features
  - 1.6) Final conclusions of preliminary analysis for Preprocessing

**Experiment phase:**
- 2) Preprocessing
  - 2.1) MEAN value for NaN-value samples
  - 2.2) Standard scaling

- 3) Metrics selection

- 4) Model training and evaluation
  - 4.1) Selections
  - 4.2) Training
  - 4.3) Evaluation on dev-set

- 5) Best model selection

- 6) Optuna hyperparameter optimization
  - 6.1) Hyperparameter tuning
  - 6.2) Visualization of hyperparameter tuning
  - 6.3) Print best paramaters
  - 6.4) Improvement on MSE
  
**Post experiment phase:**
- 7) Final evaluation on test set

###**1) Preliminary data analysis:**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""####**1.1) Print data**"""

raw_data = pd.read_pickle('/content/ass3.pickle')
print(raw_data)

# train set
X_train = raw_data['train'].iloc[:,0:8] # all features
y_train = raw_data['train']['target'] # target

# dev set
X_dev = raw_data['dev'].iloc[:,0:8]
y_dev = raw_data['dev']['target']

# test set
X_test = raw_data['test'].iloc[:,0:8]
y_test = raw_data['test']['target']

"""####**1.2) Visualization**

***Visualization of train-set for first impressions***

**Distribution of targets to samples**
"""

def plot_target_distribution(target):
    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))

    # Plot histogram
    axes[0].hist(y_train, bins=20, alpha=0.5)
    axes[0].set_xlabel('Target Variable')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Histogram of Target Variables')

    # Plot density plot
    sns.kdeplot(y_train, shade=True, ax=axes[1], fill=True)
    axes[1].set_xlabel('Target Variable')
    axes[1].set_ylabel('Density')

    plt.tight_layout()
    plt.show()

plot_target_distribution(y_train)

"""Analysis of targets distribution:
>- shows clearly that train-set is imbalanced
- most samples refers to target values in ranges of (1,2)
- least samples refers to target values around (0,1) and (3,5)

####**1.3) Range of target values**

**Distribution of each feature to targets**
"""

import matplotlib.pyplot as plt
def plot_feature_vs_target(data, target):
    num_features = data.shape[1]
    fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(8, 6*num_features))

    for i, feature in enumerate(data.columns):
        ax = axes[i]
        ax.scatter(data[feature], target, alpha=0.3)
        ax.set_xlabel(feature)
        ax.set_ylabel('Target Variable')

    plt.tight_layout()
    plt.show()

plot_feature_vs_target(X_train, y_train)

"""Analysis of features to targets distribution:


> feature 0:
- most samples are distributed around target values (0.5,2.5)

> feature 1:
- is distributed around all target values (0,6) but with higher density of samples at target values (0.5,3) and (5,6)

> feature 2:
- most samples are distributed around target values (0.5,5)

> feature 3:
- like feature 2 most samples are distributed around target values (0.5,5)

> feature 4:
- most samples are distributed around target values (0.5,3.75)

> feature 5:
- like features 2 and 3 most samples are distributed around target values (0.5,5)

> feature 6:
- most samples are distributed around target values (0.5,1)

> feature 7:
- most samples are distributed around target values (0.5,1.5)

####**1.4) Indicating presence of NaN values in dataset**
"""

def check_nan_values(dataset):
    nan_indices = np.isnan(dataset).any(axis=1)
    if np.any(nan_indices):
        print("There are samples with NaN values.")
    else:
        print("No samples with NaN values found.")

print("Check NaN values in train-set:")
check_nan_values(X_train)
print("\nCheck NaN values in dev-set:")
check_nan_values(X_dev)
print("\nCheck NaN values in test-set:")
check_nan_values(X_test)

def plot_nan_values(dataset):
    plt.figure(figsize=(10, 6))
    sns.heatmap(np.isnan(dataset), cmap='binary')
    plt.title('Presence of NaN Values')
    plt.xlabel('Features')
    plt.ylabel('Samples')
    plt.show()

plot_nan_values(X_train)

"""####**1.5) Number of samples and features**

Now get the exact numbers of distribution:
"""

# print train set
print('Total number of (samples, features) in train-set is:', X_train.shape)
print('Number of samples of each lable is:')
print(y_train.value_counts(), '\n')

# print dev set
print('Total number of (samples, features) in dev-set is:', X_dev.shape)
print('Number of samples of each lable is:')
print(y_dev.value_counts(), '\n')

# print test set
print('Total number of (samples, features) in test-set is:', X_test.shape)
print(y_test.value_counts(), '\n')

"""####**1.6) Final conclusions of preliminary analysis for Preprocessing**

> - for samples with NaN-values standardize by MEAN value in order to not loosing data
- balance train-set for regressor which are sensitive to imbalanced data

###**2) Preprocessing**
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

"""*Explanation:*
> To not loose may important data, we input mean value in NaN samples instead of removing those samples.

> Standard scaling specifically for Support Vector Regressor and generally for faster training and evaluation.

####**2.1) Replacing NaN samples with Mean**
"""

# Step 2.1: Handle samples with NaN values in the train set
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)

"""####**2.2) Standard scaling**"""

# Step 2.2: Standardize data set
scaler = StandardScaler()
# Standardize the train set
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_dev_scaled = scaler.transform(X_dev)
X_test_scaled = scaler.transform(X_test)

"""###**3) Metrics selection**

We'll evaluate due to following metrices:
- MSE (Mean squared error)
- MAE (Mean absolute error)
- R2-score
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""###**4) Model training and evaluation**

We'll train the following models:

- Linear Regressor
- Decision Tree Regressor
- Random Forest Regressor
- Gradient Boosting Regressor
- XGB Regressor
- Support Vector Regressor

Hence the dataset is provided with labels/targets we won't train clustering models.
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score

"""####**4.1) Model selections**"""

# Define the models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree Regression': DecisionTreeRegressor(),
    'Random Forest Regression': RandomForestRegressor(),
    'Gradient Boosting Regression': GradientBoostingRegressor(),
    'XGB Regressor': XGBRegressor(),
    'Support Vector Regression': SVR()
}
# Dictionary to store cross-validation results
cv_results = {}

"""####**4.2) Training**"""

# Perform cross-validation for each model
for model_name, model in models.items():
    # Perform cross-validation with neg_MSE scoring
    neg_mse_scores = -cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')

    # Perform cross-validation with R2 scoring
    r2_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')

    # Perform cross-validation with neg_MAE scoring
    mae_score = -cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_absolute_error')

    # Save the cross-validation results
    cv_results[model_name] = {'MSE': neg_mse_scores, 'R2': r2_scores}

    # Print the cross-validation results
    print(f"Model: {model_name}")
    print(f"Cross-Validation MSE scores: {neg_mse_scores}")
    print(f"Average MSE: {neg_mse_scores.mean()}")
    print(f"Cross-Validation R2 scores: {r2_scores}")
    print(f"Average R2: {r2_scores.mean()}")
    print(f"Cross-Validation MAE scores: {mae_score}")
    print(f"Average MAE scores: {mae_score.mean()} \n")

"""####**4.2) Evaluation of models on dev-set**"""

dev_results = []

# Perform evaluation on the dev set for each model
for model_name, model in models.items():
    model.fit(X_train_scaled, y_train)  # Fit the model on the entire training set

    # Make predictions on the scaled dev set
    y_pred_dev = model.predict(X_dev_scaled)

    # Calculate MSE, MAE and R2 score on the dev set
    mse_dev = mean_squared_error(y_dev, y_pred_dev)
    mae_dev = mean_absolute_error(y_dev, y_pred_dev)
    r2_dev = r2_score(y_dev, y_pred_dev)

    # Save the evaluation results on the dev set
    dev_results.append({'Model': model, 'MSE': mse_dev,'MAE':mae_dev ,'R2': r2_dev})
    # Print the evaluation results on the dev set
    print(f"Model: {model_name}")
    print(f"Dev Set MSE: {mse_dev}")
    print(f"Dev Set MAE: {mae_dev}")
    print(f"Dev Set R2 Score: {r2_dev} \n")

"""##**5) Best model selection**"""

# Sort models based on MSE on the dev set in ascending order
sorted_models_dev = sorted(dev_results, key=lambda x: x['MSE'])

# Select the best model based on MSE on the dev set
best_model = sorted_models_dev[:1]
best_model = best_model[0]['Model']

print(f'Best model on dev-set is \n')
print(f"Model: {best_model}")
print(f"MSE: {model['MSE']}")
print(f"MAE: {model['MAE']}")
print(f"R2 Score: {model['R2']} \n")

"""*Explanation:*


> As expexted, the Extreme Gradient Boost Regressor performed best on dev-set.
We'll now tune it's hyperparamters and refit it on train-set in order to check whether an improvement on MSE performance can be achieved.

##**6) Optuna hyperparameter optimization**
"""

!pip install optuna
import optuna

"""####**6.1) Hyperparameter tuning**




"""

best_params = None
best_mse = 0.0

def objective(trial):
    global best_model
    global best_params
    global best_mse

    # Define the hyperparameter search spaces for the second model (Gradient Boosting)
    XGBR_params = {
    'n_estimators': trial.suggest_float('gb_n_estimators', 100, 500, step=100, log=True),
    'learning_rate': trial.suggest_loguniform('gb_learning_rate', 0.001, 0.1),
    'max_depth': trial.suggest_int('gb_max_depth', 3, 10),
    'min_samples_leaf': trial.suggest_int('gb_min_samples_leaf', 1, 10),
}

    best_model.set_params(**XGBR_params)


    # Refit on train set
    best_model.fit(X_train_scaled, y_train)

    # Evaluate models ondev-set
    y_dev_pred_best_model = best_model.predict(X_dev_scaled)
    best_model_mse_dev = mean_squared_error(y_dev, y_dev_pred_best_model)


    # Update the best model and parameters if necessary
    if best_model_mse_dev < best_mse:
        best_params = XGBR_params
        best_mse = best_model_mse_dev

    # Return the minimum MSE of the two models
    return best_mse

# Create the Optuna study
study = optuna.create_study(direction='minimize')

# Start the hyperparameter optimization
study.optimize(objective, n_trials=100)

"""####**6.2) Visualization of hyperparamter tuning**




"""

from optuna.visualization import plot_optimization_history, plot_slice, plot_param_importances
plot_optimization_history(study)
# Visualize the hyperparameter slice
plot_slice(study)
# Visualize the importance of hyperparameters
plot_param_importances(study)

"""####**6.3) Print best hyperparamters**




"""

# Get the best hyperparameters
best_params = study.best_params
print(f'Best Hyperparameters: {best_params}')

"""#### **6.4) Improvement on MSE after uptuna tuning**"""

prev_MSE = model['MSE']
print(f'Improvement on MSE is {abs(best_mse - prev_MSE)} ')

"""###**7) Final evaluation on test set**

"""

best_model.set_params(**best_params)
best_model.fit(X_train_scaled, y_train)

y_pred_dev = best_model.predict(X_dev_scaled)

# Evaluate the best model on the final test set
y_test_pred = best_model.predict(X_test_scaled)
mse_test = mean_squared_error(y_test, y_test_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

# Print the evaluation results on the test set
print(f"Performance of {best_model} on test set: \n")
print(f"MSE on Test set: {mse_test}")
print(f"MAE on Test set: {mae_test}")
print(f"R2 Score on Test set: {r2_test}")

"""**Final analysis of performance on test-data:**

> With an MSE value of 0.21942, MSA value of 0.30344 and R2-score of 0.84269 we got overall a got perfromance of the XGB Regressor on new, unseen data.




"""